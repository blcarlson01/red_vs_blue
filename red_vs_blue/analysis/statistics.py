"""
statistics.py

Statistical significance testing for the Red vs. Blue benchmark.
Reads from aggregated results (generated by aggregate_results.py).
"""

from __future__ import annotations
import sys
from pathlib import Path
from itertools import combinations
from typing import Dict, Tuple

import numpy as np
import pandas as pd

from red_vs_blue.metrics import (
    permutation_test,
    cohens_d,
    holm_bonferroni,
)


# ============================================================
# Configuration
# ============================================================

METRICS = {
    "win_rate": "value",
    "belief_alignment": "avg_belief_alignment",
    "entropy_reduction": "avg_entropy_reduction",
    "brier": "avg_brier",
    "apt_leader_deception": "apt_leader_deception",
    "rounds": "avg_rounds",
}

ALPHA = 0.05


# ============================================================
# Helpers
# ============================================================

def load_results(results_dir: Path) -> pd.DataFrame:
    """
    Load flattened per-game results from aggregated JSONL.
    Requires aggregate_results.py to be run first.
    """
    path = results_dir / "aggregated" / "all_results.jsonl"
    if not path.exists():
        raise FileNotFoundError(
            f"Aggregated results not found. Run aggregate_results.py first (missing {path})"
        )
    return pd.read_json(path, lines=True)


def compare_models(
    df: pd.DataFrame,
    model_a: str,
    model_b: str,
) -> Dict[str, Dict]:
    """
    Compare two models across all metrics.
    """
    results = {}

    for name, col in METRICS.items():
        xa = df[df.model == model_a][col].dropna().values
        xb = df[df.model == model_b][col].dropna().values

        if len(xa) < 5 or len(xb) < 5:
            continue

        p = permutation_test(xa, xb)
        d = cohens_d(xa, xb)

        results[name] = {
            "mean_a": float(np.mean(xa)),
            "mean_b": float(np.mean(xb)),
            "p_value": p,
            "effect_size_d": d,
        }

    return results


def print_results(
    model_a: str,
    model_b: str,
    raw_results: Dict[str, Dict],
):
    """
    Apply Holm–Bonferroni and print results.
    """
    pvals = {k: v["p_value"] for k, v in raw_results.items()}
    hb = holm_bonferroni(pvals, alpha=ALPHA)

    print(f"\n=== {model_a} vs {model_b} ===\n")

    for metric, stats in raw_results.items():
        hb_info = hb.get(metric, {})
        sig = hb_info.get("reject_null", False)

        star = " *" if sig else ""
        direction = ">" if stats["mean_a"] > stats["mean_b"] else "<"

        print(
            f"{metric:20s}: "
            f"{stats['mean_a']:.3f} {direction} {stats['mean_b']:.3f} | "
            f"p={stats['p_value']:.4f}, "
            f"d={stats['effect_size_d']:.2f}"
            f"{star}"
        )

    print("\n* Significant after Holm–Bonferroni (α=0.05)\n")


# ============================================================
# Main
# ============================================================

def main(results_dir: str):
    results_path = Path(results_dir)
    
    try:
        df = load_results(results_path)
    except FileNotFoundError as e:
        print(f"Error: {e}")
        print("\nPlease run aggregate_results.py first:")
        print(f"  python aggregate_results.py {results_dir}")
        sys.exit(1)

    if df.empty:
        print("No results found.")
        return

    models = sorted(df.model.unique())

    if len(models) < 2:
        print(f"Only {len(models)} model(s) found. Need at least 2 for comparison.")
        return

    print("\nModels:", ", ".join(models))

    for model_a, model_b in combinations(models, 2):
        raw = compare_models(df, model_a, model_b)
        if raw:
            print_results(model_a, model_b, raw)

if __name__ == "__main__":
    if len(sys.argv) != 2:
        print("Usage: python statistics.py <results_dir>")
        sys.exit(1)

    main(sys.argv[1])
